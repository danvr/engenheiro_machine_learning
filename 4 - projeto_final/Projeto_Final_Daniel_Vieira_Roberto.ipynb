{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nanodegree Engenheiro de Machine Learning\n",
    "\n",
    "## Projeto Final Capstone\n",
    "\n",
    "### Otimização de Campanha de Marketing via Clusterização e Regressão \n",
    "\n",
    "O objetivo do projeto é aplicar o aprendizado obtido durante o programa de Nanodegree Engenheiro de Machine Learning para solucionar um problema do mundo real em uma área de interesse ao aplicar algoritmos e técnicas de machine learning.\n",
    "\n",
    "* Definicão do problema;\n",
    "* Análise Exploratória de dados;\n",
    "* Implementação do algoritmo;\n",
    "* Avalição e Validação do modelo;\n",
    "* Resultados;\n",
    "* Conclusões;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicão do problema\n",
    "No Marketing digital, uma atividade muito comum é a utilização de ferramentas de anúncios que realizam campanhas tanto em sites de busca (Google, Bing, etc) ou em redes sociais (Facebook, Instagram,etc), em resumo, esses anúncios aparecem para as pessoas que estão buscando algum serviço em questão. As plataformas fornecem opções de filtros para que esses anúncios tenham um público-alvo espesifico, a pergunta feita é como as empresas sabem o perfil desses público alvo? Esse é o problema em questão que esse projeto irá ajudar a resolver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conjunto de Dados e Inputs\n",
    "\n",
    "Os dados usados neste projeto foram encontrados no banco de dados da kaggle para infomações e download do dataset está [aqui.](https://www.kaggle.com/loveall/clicks-conversion-tracking)\n",
    "Os dados se referem a uma da campanha publicitária de mídia social de uma organização anônima. O Dataset contém 1143 observações (Linhas) em 11 variáveis (Colunas). Abaixo estão as descrições das variáveis:\n",
    "\n",
    "* **ad_id:** ID exclusivo para cada anúncio; \n",
    "* **xyz_campaign_id:** ID associado a cada campanha publicitária empresa XYZ;\n",
    "* **fb_campaign_id:** ID associado a campanha como o facebook rastreia a camapanha;\n",
    "* **age:** Idade da pessoa a quem o anúncio foi mostrado; \n",
    "* **gender:** Gênero da pessoa a quem o anúncio foi mostrado;\n",
    "* **interest:** Código que especifica a categoria de qual o interesse da pessoa pertence; \n",
    "* **Impressions:** Numero de vezes que aunncio foi mostrado;\n",
    "* **Clicks:** Número de cliques do anuncio;\n",
    "* **Spent:** Quantidade pago pela empresa XYZ para o Facebook, para mostrar o anúncio;\n",
    "* **Total conversion:** Número total de pessoas que se interessaram sobre o produto ou serviço depois de ver o anúncio;\n",
    "* **Approved conversion:** Número total de pessoas que compraram o produto depois de ver o anúncio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise Exploratória de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importe as biblotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import seaborn as sns\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cluster\n",
    "from sklearn import decomposition\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrege os dados\n",
    "data = pd.read_csv('KAG_conversion_data.csv')\n",
    "# Criando uma copia dos dados (Boa Prática)\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visão geral dos dados\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obs 1:**  Aparentemente não tem nenhum dado faltando, os tipos de dados deveram ser investigados para validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspecione as 5 primeiros observações\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecionando os primeiros elementos, podemos perceber que a colunas 'age' e 'gender' são categóricas, apesar que a coluna 'age' ser numérica, ela representa uma categoria de uma faixa de idade, vamos investigar se alguma variável numérica é na verdade uma categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PLote grárfico de barra horizontal da quantidade de valores unicos de todas as variavéis númericas\n",
    "plt.figure(figsize=[5,5])\n",
    "for column in df._get_numeric_data():\n",
    "    unique_prop = (len(df[column].unique())/df[column].size)*100\n",
    "    plt.barh(y = column, width=unique_prop,color = 'b', alpha = 0.8)\n",
    "plt.title(\"Contagem dos Valores únicos\")\n",
    "plt.xlabel(\"Soma\")\n",
    "plt.ylabel(\"Variáveis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A visualização acima indica que 'Total_Conversion', 'Approved_Conversion', 'interest', 'xyz_campaign_id' podem ser categóricas, segundo o meta dados fornecidos,'Total_Conversion' e 'Approved_Conversion' seriam número total de conversões então seriam realmente dados numéricos, 'interest', 'interest' seriam códigos que especificam as categoria de interesse esses terão que ser alterados e 'xyz_campaign_id' seria o código da campanha da empresa XYZ também deverá ser alterado. 'ad_id'tem 100% de valores únicos o que significa que está servido como índice, vamos usar o índice do DafaFrame, estão essa coluna deverá ser removida, Em resumo, **será alterado o tipo das variáveis 'interest' e 'xyz_campaign_id' de numérico para categórico e 'ad_id' será removido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Faça uma tabela frequência de 'xyz_campaign_id'\n",
    "freq_table = pd.crosstab(df['xyz_campaign_id'],columns= 'Count')\n",
    "freq_table['%'] = np.round(freq_table/freq_table.sum(),3)\n",
    "freq_table.sort_values('%',ascending = False,inplace = True)\n",
    "\n",
    "# Plote um gráfico de barras vertical\n",
    "ax = freq_table['Count'].plot(kind= \"bar\",figsize=(10,7))\n",
    "\n",
    "# Amazene o falor total das colunas a serem plotadas\n",
    "total = freq_table[\"Count\"].sum()\n",
    "\n",
    "# Configure as legendas das colunas, eixos e titulo.\n",
    "for i in ax.patches:\n",
    "    # Ajuste a posição e tamanho da fonte da legenda\n",
    "    ax.text(i.get_x()+.10, i.get_height()+.10, \\\n",
    "            str(round((i.get_height()/total)*100, 2))+'%', fontsize=15,\n",
    "                color='dimgrey')\n",
    "plt.title(\"Tabela Frequência de 'xyz_campaign_id'\")    \n",
    "plt.ylabel(\"Contagem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variável 'xyz_campaign_id' existem apenas 3 valores únicos, o que confirma que pode ser lidado como uma variável categórica, sendo divido entre as campanhas 1178 e 936, a campanha 916 representa apenas 4.72%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Faça uma tabela frequência de 'interest'\n",
    "freq_table = pd.crosstab(df['interest'],columns= 'Count')\n",
    "freq_table['%'] = np.round(freq_table/freq_table.sum(),3)\n",
    "freq_table.sort_values('%',ascending = False,inplace = True)\n",
    "\n",
    "# Plote um gráfico de barras horizontal\n",
    "ax = freq_table['Count'].plot(kind= \"barh\",figsize=(12,10))\n",
    "\n",
    "# amazene o falor total das colunas a serem plotadas\n",
    "total = freq_table[\"Count\"].sum()\n",
    "\n",
    "# Configure as legendas das colunas, eixos e titulo.\n",
    "for i in ax.patches:\n",
    "    # get_width pulls left or right; get_y pushes up or down\n",
    "    ax.text(i.get_width()+.4, i.get_y()+.42, \\\n",
    "            str(round((i.get_width()/total)*100, 2))+'%', fontsize=13,\n",
    "color='dimgrey')\n",
    "ax.invert_yaxis()\n",
    "plt.title(\"Tabela Frequência de 'interest\")    \n",
    "plt.ylabel(\"Código Interest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apesar de a distribuição sugere que é uma variável numérica, de acordo com os meta dados fornecidos `'interest'` é um código que especifica a categoria de qual o interesse da pessoa pertence, então vamos tratar-lo como uma variável categoria extensa, podemos essa variável como a matriz características dos clusteres.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plote uma grafico e frequências das variáveis categoricas\n",
    "plt.figure(figsize=[5,5])\n",
    "cat_col = ['age','gender']\n",
    "for column in cat_col:\n",
    "    plt.figure()\n",
    "    plt.title('Frequência \"{}\"'.format(column))\n",
    "    plt.xlabel(\"Frequência\")\n",
    "    plt.ylabel(\"Categoria\")\n",
    "    freq_table = pd.crosstab(df[column],columns= ['Frequency(%)'], normalize = True)\n",
    "    freq_table.sort_values('Frequency(%)',ascending = True,inplace = True)\n",
    "    plt.barh(y = freq_table.index, width=freq_table['Frequency(%)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos afirmar que pelo menos 80% dados são representados por pessoas de 30 a 39 anos, e é dividido entre homens e mulheres, com a maioridade com idade entre 30 a 34 anos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostre as estatiscas descritivas\n",
    "df.describe().iloc[:,5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que em 'Clicks', 'Spent', 'Total_Conversion', 'Approved_Conversion' apresentam uma grande diferença entre 75 percentil para o valor máximo, isso significa se ordenarmos nossos dados de forma crescente 75% deles seriam menor que o valor apresentado no linha \"75%\", no caso de \"Clicks\", seria 37.5 e ultimo valor máximo, o último valor dos dados ordenados, seria 421, é notável que houve um crescimento fora do padrão, considerando que ele saiu de 0 a 35.5 recorrendo 75%, claramente temos  outlies, teremos que investigar mais essa variáveis de forma visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plote uma matriz de scatterplots e histogramas\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora ficou claro na presença de outlies nas variáveis `'Clicks'`, `'Spent'`, `'Total_Conversion'`, `'Approved_Conversion'` conforme mostra a curva assimétrica direita, confirmando a análise anterior, além disso todas elas aparetam ter alguma fortes correlações.\n",
    "\n",
    "É possivel confirmar que a variável 'interest' é categorica por apresentar nenhuma correlacão com as demais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para visuzliar melhor as correlações, plote um mapa de calor das mesmas.\n",
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem algumas variáveis que são fortemente correlacionadas, isso é notavel quando visualizamos os gráficos de dispersão que aparentam ser uma linha, que em outras palavras , indica o quanto a variável explica o comportamento linear da outra, para um problema de aprendizagem não supervisionada isso não é incessante, que implica que as variáveis tem a mesma informação, teremos que criar novas variáveis de forma que diminua essa coração e aumente a informação.\n",
    "\n",
    "'fb_campaign_id' é muito correlacinada com 'ad_id', como anteriormente havimos definido a remocão de 'ad_id', será removido também 'fb_campaign_id'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento dos dados\n",
    "Para organizar o Pré-processamento dos dados, essa atividade foi dividida 4 em partes.\n",
    "\n",
    "**1 -** Alterar o tipo das variáveis 'interest' e 'xyz_campaign_id' de numérico para categórico e remoção 'ad_id' e 'fb_campaign_id'\n",
    "\n",
    "**2 -** Criar novas variáveis utilizando 'Clicks' , 'Spent','Impressions'  e remover as variáveis numéricas inicias.\n",
    "\n",
    "**3 -**  Escalonamento e Tratamento de Outlies.\n",
    "\n",
    "**4 -** Seleção de atributos (Feature Selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 -  Alterar o tipo das variáveis 'interest' e 'xyz_campaign_id' de numérico para categórico e remoção 'ad_id' e 'fb_campaign_id'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Altere para 'category' todas as variáveis para manter um padrão\n",
    "df[['xyz_campaign_id','age','interest','gender']] = df[['xyz_campaign_id','age','interest','gender']].astype('category')\n",
    "\n",
    "# Remova'ad_id' e 'fb_campaign_id'\n",
    "df.drop(['ad_id','fb_campaign_id'], axis = 1, inplace = True)\n",
    "\n",
    "# Slave as variávies categoricas\n",
    "cat_var = df.select_dtypes(include = 'category').columns.tolist()\n",
    "\n",
    "# Amazene as variáveis numéricas\n",
    "num_var = df._get_numeric_data().columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecione o resultado\n",
    "display(df.info())\n",
    "print(\"Lista das variáveis categóricas:\")\n",
    "display(cat_var)\n",
    "print(\"Lista das variáveis numéricas:\")\n",
    "display(num_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Criar nova variável utlizando 'Clicks' , 'Spent','Impressions'  e tratar outlies se necessário.\n",
    "\n",
    "Seram criados alguns KPI padrões que são utilizados no marketing digital:\n",
    "\n",
    "**Taxa de cliques \"Click-Through-Rate\" (CTR):** Esta é a porcentagem de quantas impressões se tornaram cliques. Mede a atratividade do anúncio. Um Benchmark para essa KPI seria 2% para ser razoável.\n",
    "\n",
    "**Taxa de Conversão \"Conversion-Rate\" (CR):** Esta é a porcentagem de cliques que resultam em uma \"conversão\". Conversão é determinado por um objetivo que é definido para a campanha. O que mede efetivamente a efetividade anúncio de campanha. Para dataset do projeto, serão criados 2 taxas CR por temos 2 variáveis distintas de conversão, uma que o objetivo é despertar o interesse e a outra em comprar o produto/serviço.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Taxa de cliques \"Click-Through-Rate\"\n",
    "df['CTR'] = (df['Clicks'] / df['Impressions'])*100\n",
    "\n",
    "# Taxa de Conversão \"Conversion-Rate\" para 'Approved_Conversion'\n",
    "df['ACR'] = ((df['Approved_Conversion'])/ df['Clicks'])*100\n",
    "\n",
    "# Taxa de Conversão \"Conversion-Rate\" para 'Total_Conversion'\n",
    "df['TCR'] = ((df['Total_Conversion'])/ df['Clicks'])*100\n",
    "\n",
    "\n",
    "df.replace([np.inf, -np.inf], [100,0], inplace = True)\n",
    "\n",
    "# Atualize Num_var\n",
    "num_var= [\"CTR\",\"ACR\",\"TCR\"]\n",
    "\n",
    "df[num_var] = df[num_var].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df._get_numeric_data().corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Problema das correlações não foi resolvido e temos outlies, mas tivemos um bom resultado com as distribuições e correlações das variáveis criadas. Como temos 3 Variáveis altamente correlacionadas entre elas, e algorimitmos de clusterização cometer erros por viés, devemos escolher uma, como estamos objetivando melhorar a taxa de converção, a variável mais adequeada entre as 3, seria a \"Impressions\", de forma de nos entendermos como o comusmidor se comportar ao clicar nos anuncios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 -  Tratando Outlies\n",
    "\n",
    "Essa atividade será divida em 3 partes:\n",
    "\n",
    "- Identificação de outliers: Será utilizado Bloxplot para visualizar esses pontos, lembrando que Boxplot por padrão, identifica como outlier as pontos que estão a 1.5 desvios padrões do IQR(Intervalo inter Quartil).\n",
    "- Análise: Será avaliado se esses pontos fazem sentido, ver a representatividade  dos dados\n",
    "- Tratamento: Será decidido os pontos serram mantidos, alterados ou removidos nessa ordem de importância, o escalonamento deve acontecer para reduzir também reduzir o efeito de outlies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Indentificacão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plote um boxplot das variáveis numéricas\n",
    "sns.boxplot(data = df[num_var])\n",
    "plt.title(\"Indentificando Outlies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aparemente apenas \"CTR\" tem alguns outlies, devemos pegar os seus indices e explorálos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Encontre os outlies, mostre e amazerne os índices\n",
    "out_idx = []\n",
    "\n",
    "\n",
    "for feature in num_var:\n",
    "       \n",
    "    # Calule a faxa onde estão os outlies (+- 1.5*IQR)\n",
    "    Q1 =  df[feature].quantile(.25)\n",
    "    Q3 = df[feature].quantile(.75)\n",
    "    IQR = Q3- Q1\n",
    "    out_min = Q1 - 1.5*IQR\n",
    "    out_max = Q3 + 1.5*IQR\n",
    "\n",
    "    # Amarzene os indices dos outlies   \n",
    "    idx = df[feature][(df[feature] <= out_min) | (df[feature] >= out_max)].index.tolist()\n",
    "    out_idx += idx\n",
    "\n",
    "    # Calcule a proporção dos outlies\n",
    "    out_p = np.round((len(idx) / len(df)) * 100,2)\n",
    "\n",
    "    # Mostre para inspeção os outlies encontrados\n",
    "    print(\"{} ({}%) observações consideradas outlies da variável {}\".format(len(idx),out_p,feature))\n",
    "    display(df.loc[idx].head(3))\n",
    "    display(df.loc[idx,feature].describe())\n",
    "\n",
    "# Transfore a lista em um Panda Series\n",
    "out_idx = pd.Series(out_idx)\n",
    "    \n",
    "# Encontre os valores duplicados\n",
    "dup_idx = out_idx.iloc[np.where(pd.Series(out_idx).duplicated(keep=False))[0]]\n",
    "dup_idx = dup_idx.unique()\n",
    "\n",
    "# Encontre os valores únicos\n",
    "out_idx = out_idx.unique()\n",
    "\n",
    "# Calcule a representação total dos outlies no data set\n",
    "out_p = np.round((len(out_idx) / len(df)) * 100,2)\n",
    "dup_p = np.round((len(dup_idx) / len(df)) * 100,2)\n",
    "\n",
    "# Mostre a quantidade total de outlies\n",
    "print(\"No total, {} ({}%) linhas são outlies unicos, sendo {} ({}%) em pelo menos 1 variável\".format(len(out_idx),out_p,\n",
    "                                                                                                   len(dup_idx),dup_p)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe  uma quantidade expressiva de outliers, remover-los irá impacatar demais no dataset, além disso, eles aparatam ser dados legitimos, talvez algumas camapanhas que estão rodando a mais tempo e por isso receberam mais impressions e clicks. Esses outlies seram tratados de uma forma matmática, de forma que seram colocados em uma escalados logarítima, onde irá diminuir a diferença dos valores extremos, mas primeiro temos que tratar os zeros do data set, na escala logaritima, a valor zero vale menos infito ( - inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[num_var].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substitua os zeros do dataset por um número muito pequeno (0,01)\n",
    "for column in df[num_var].columns:\n",
    "    df.loc[df[column] <= 0.01,column] = 0.1\n",
    "\n",
    "# Transforme as novas numéricas (velhas e novas) na escala logarítima\n",
    "df[num_var] = np.log(df[num_var] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[num_var].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(data = df[num_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Encontre os outlies, mostre e amazerne os índices\n",
    "out_idx = []\n",
    "\n",
    "for feature in num_var:\n",
    "       \n",
    "    # Calule a faxa onde estão os outlies (+- 1.5*IQR)\n",
    "    Q1 =  df[feature].quantile(.25)\n",
    "    Q3 = df[feature].quantile(.75)\n",
    "    IQR = Q3- Q1\n",
    "    out_min = Q1 - 1.5*IQR\n",
    "    out_max = Q3 + 1.5*IQR\n",
    "\n",
    "    # Amarzene os indices dos outlies   \n",
    "    idx = df[feature][(df[feature] <= out_min) | (df[feature] >= out_max)].index.tolist()\n",
    "    out_idx += idx\n",
    "\n",
    "    # Calcule a proporção dos outlies\n",
    "    out_p = np.round((len(idx) / len(df)) * 100,2)\n",
    "\n",
    "    # Mostre para inspeção os outlies encontrados\n",
    "    print(\"{} ({}%) observações consideradas outlies da variável {}\".format(len(idx),out_p,feature))\n",
    "    display(df.loc[idx].head(3))\n",
    "    display(df.loc[idx,feature].describe())\n",
    "\n",
    "# Transfore a lista em um Panda Series\n",
    "out_idx = pd.Series(out_idx)\n",
    "    \n",
    "# Encontre os valores duplicados\n",
    "dup_idx = out_idx.iloc[np.where(pd.Series(out_idx).duplicated(keep=False))[0]]\n",
    "dup_idx = dup_idx.unique()\n",
    "\n",
    "# Encontre os valores únicos\n",
    "out_idx = out_idx.unique()\n",
    "\n",
    "# Calcule a representação total dos outlies no data set\n",
    "out_p = np.round((len(out_idx) / len(df)) * 100,2)\n",
    "dup_p = np.round((len(dup_idx) / len(df)) * 100,2)\n",
    "\n",
    "# Mostre a quantidade total de outlies\n",
    "print(\"No total, {} ({}%) linhas são outlies unicos, sendo {} ({}%) em pelo menos 1 variável\".format(len(out_idx),out_p,\n",
    "                                                                                                   len(dup_idx),dup_p)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos percerber, a técnica aplicada foi extremamente eficaz, nossa proporção de outlies cairam de 41.99%% para 9.62%, apesar de quase 10% é o valor expresivo, o fato de após ter colocato em escala logarítima as os valores ainda se apresetam como outlies, eles deveram ser removidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(out_idx,inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corr = df._get_numeric_data().corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após o tratamento dos outlies, as correlações melhoraram bastante, podemos procesguir com o pre processamento dos dados para treinar o algoritimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para melhorar a performance do modelo e evitar que ele possa sofrer erro por viés de uma variável que esteja altamente correlacionada, iremos testar as variáveis numéricas usando a seguinte metodologia:\n",
    "\n",
    "Vamos remover uma variável numérica e treinar um modelo de regressão para tentar prever utilizando as variáveis restantes, iremos validar o modelo utilizando a métrica R^2 ( coeficiente de determinação) que mede o quão bom está o modelo, se o modelo tiver um alto R2 significa que a variável está perfeitamente sendo prevista , o que implica que ela não é relevante para identificar caraterísticas únicas, já que outras variáveis tenham a mesma informação que a mesma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use sklearn to perform a train-test split on data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "for columun in num_var:\n",
    "    X = df[num_var].drop(columun, axis =1).values\n",
    "    y = df[columun].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state =15)\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    # Get Predictions\n",
    "    predictions = model.predict(X_test)\n",
    "          \n",
    "    # Print the results\n",
    "    print('R^2: ' + str(columun) +\" \"+ str(r2_score(y_test, predictions)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não há nenhuma variável que foi completamente prevista pelas outras, o que significa que o modelo corre menos risco de sofrer erro de viés por alguma variável.\n",
    "\n",
    "O mesmo modelo de teste será realizado com as variáveis categorias, mas agora utilizando o teste de independência Qui-Quadrado é usado para descobrir se existe uma relação entre as variáveis categóricas, enquanto menor o valor (próximo de zero) siginfica que uma váreial está extremamente relacionada a outra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "chi_matrix = []\n",
    "for category_1 in cat_var:\n",
    "    chi_list = []\n",
    "    for category_2 in cat_var:\n",
    "        observed = pd.crosstab(df[category_1],df[category_2])\n",
    "        chi_list.append(chi2_contingency(observed)[1])\n",
    "        \n",
    "    chi_matrix.append(chi_list)\n",
    "chi_matrix = pd.DataFrame(chi_matrix,index= cat_var,columns=cat_var)\n",
    "chi_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliando os resultados do teste Qui-Quadrado, pode observar que as variáveis \"xyz_campaign_id\" e \"gender\" tem relação entre todas as variáveis, sendo assim, selas seram removidas para que o modelo não sofra viés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atualize cat_var\n",
    "cat_var = df.drop([\"xyz_campaign_id\",\"gender\"],axis =1).select_dtypes(include = 'category').columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Kprototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Selecione as colunas que seram treinadas\n",
    "train_clolmuns = num_var + cat_var\n",
    "\n",
    "# Transforme os dados em arrays para melhor processamento\n",
    "X = df[train_clolmuns].values\n",
    "\n",
    "# Crie uma lista para amarzenar os valores do coeficiente shioueta\n",
    "avg_silhouettes = [] \n",
    "\n",
    "# Treine modelo com 2 a 6 clusters\n",
    "for k in range(2,6):\n",
    "        kp = KPrototypes(n_clusters = k, random_state=28) # Crie a instância\n",
    "        cluster_labels = kp.fit_predict(X,categorical=[3,4]) # Pege os labels dos clusters\n",
    "        shihouette_avg = metrics.silhouette_score(X[:,:-2], cluster_labels) # Calcule o coeficiente de silhueta\n",
    "        avg_silhouettes.append(shihouette_avg) # Amarzene o coeficiente de silhueta\n",
    "        \n",
    "# Plote o gráfico de silhueta        \n",
    "plt.figure() \n",
    "plt.title(\"Coeficiente de Silhueta K-Protopypes\")\n",
    "plt.xlabel(\"Número de K\")\n",
    "plt.ylabel(\"Valor Silhueta\")\n",
    "plt.xticks(np.arange(2, 6, step=1))\n",
    "plt.plot(list(range(2,6)),avg_silhouettes)\n",
    "\n",
    "# Crie e treine o modelo otimizado\n",
    "kp = KPrototypes(n_clusters= np.argmax(avg_silhouettes) + 2,random_state=28)\n",
    "df['cluster'] = kp.fit_predict(X,categorical=[3,4])\n",
    "\n",
    "# Reduza para 2 dimensões\n",
    "pca_column = num_var\n",
    "pca = decomposition.PCA(n_components=2,random_state=28)\n",
    "df['pc1'], df['pc2'] = zip(*pca.fit_transform(df[pca_column]))\n",
    "\n",
    "\n",
    "# Crie listas para configurar os marcadores e cores das visualização\n",
    "plt.figure()\n",
    "markers = [\"o\",\"s\",\"^\",\"8\",\"x\"]\n",
    "colors =  [\"b\",\"g\",\"r\",\"c\",\"m\"]\n",
    "\n",
    "# Crie um loop para plotar os cluters na dimesão reduzida\n",
    "plt.figure()\n",
    "for c in df['cluster'].unique():\n",
    "    table = df[df['cluster'] == c]\n",
    "    plt.scatter(table['pc1'],table['pc2'], marker=markers[c], color=colors[c])\n",
    "    plt.title('K = {}'.format(np.argmax(avg_silhouettes) + 2))\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "print(\"{}% da variância é explicada por PC1 e {}% por PC2 totalizando {}% da variância dos dados\".format(round(pca.explained_variance_ratio_[0]*100,2),round(pca.explained_variance_ratio_[1]*100,2),\n",
    "                                                                                                       round(sum(pca.explained_variance_ratio_)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"O modelo final de clusterização obeteve o Coeficiente de Silhueta: \" + str(np.max(avg_silhouettes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando os resultado, é possível identificar facilmente os 3 grupos , essa visualização está bastante confiável por representar 86% da variância dos dados.\n",
    "\n",
    "Para Estudar qual cluster é mais interessante para o negócio, uma regressão linear predizendo o *'ACR'* e analisar os seus coeficientes.\n",
    "\n",
    "Primeiro devemos voltar os dados que foram colocados na escala logarítima para eu estado normal utilizando da função exponencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Retonar a escala original \n",
    "df[num_var] = np.exp(df[num_var])\n",
    "df[num_var].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos o PCA para visualisar os nosso dados, agora podemos remover essas colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizando\n",
    "df.drop([\"pc1\",\"pc2\"], axis =1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.drop(\"cluster\", axis = 1).select_dtypes(exclude = [\"uint8\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender qual o melhor cluster, vamos criar uma nova variável **ROAS** (Retorno Sobre o Investimento Publicitário), que seria a métrica que avalia a viabilidade econômica da campanha, ela é calculada pela razão da receita pelo investimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calule o ROAS\n",
    "df[\"ROAS_rate\"] = (df[\"Approved_Conversion\"] /df[\"Spent\"])*100\n",
    "\n",
    "# Ajuste os dados que sofreram uma diviSão por 0\n",
    "df.replace([np.inf, -np.inf], [1,0], inplace = True)\n",
    "df[\"ROAS_rate\"] = df[\"ROAS_rate\"].fillna(0)\n",
    "df['ROAS_rate'] = df['ROAS_rate'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o melhor entendimento dos dados, vamos explorar raptamentes as variáveis de cada cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criado array com as médias de cada coluna\n",
    "total_mean = df.drop([\"cluster\"], axis = 1).select_dtypes(exclude = [\"uint8\",\"category\"]).mean()\n",
    "total_mean = np.log(total_mean)\n",
    "# Criando visualização com a diferença de cada coluna\n",
    "for i in range(3):\n",
    "    plt.figure(i,figsize = [12,4])\n",
    "    plt.ylim(-3,2)\n",
    "   \n",
    "    plt.title(\"Cluster {}\".format(i))\n",
    "    df_cluster = df.drop(\"cluster\", axis = 1).select_dtypes(exclude = [\"uint8\"])[df[\"cluster\"]==i]\n",
    "    cluster_mean = df_cluster.mean()\n",
    "    cluster_mean = np.log(cluster_mean)\n",
    "    \n",
    "    cluster_diff = cluster_mean - total_mean\n",
    "    plt.bar(cluster_diff.index, cluster_diff)\n",
    "    plt.xlabel('Variavél')\n",
    "    plt.ylabel('Diferença da média')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível perceber que o cluster 2 tem uma grande disparidade com a média geral em relação ao *'ACR'* e fica muito próximo da média nas outras variáveis, fica claro que esse grupo não tem nenhuma `'Approved_Conversion'`. No cluster 0 e 1, temos `'Approved_Conversion'` sendo o 1 com valores muito acima da média porém com custo maior e `ACR`, podendo ser interpretado casos pouco eficiência já que se precisou de muito mais 'clicks' e consequentemente mais `'Spent'`, o cluster 0 temos bem menos  de média em `'Approved_Conversion'` porém ao considerável menor custo, indicando uma boa eficiência O ROAS do cluster 0 e 1 estão opostos (acima e abaixo da média), inicialmente podemos acreditar que o cluster 0 é o mais indicado para receber investimento já que mostra que o ROAS acima da média."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tratar os clusters como categorias e por conta disso vamos utilizaar a técnica de one hot encondig para transformar em diferentes variáveis binárias, dessa forma é possivel incluir no modelo de regressão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regressão Linear e Árvore de Decisão Aleatória."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie variável dumie para os clusters e remova 1\n",
    "df = pd.concat([df,pd.get_dummies(df['cluster']).drop(2, axis =1)],axis = 1)\n",
    "# Remova A coluna \"Cluster\"\n",
    "df.drop(\"cluster\", axis = 1, inplace = True)\n",
    "df['ROAS_rate'] = df['ROAS_rate'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecione as variáveis preditores e alvo\n",
    "training_columns = [\"Spent\",\"Approved_Conversion\",\"ACR\",\"CTR\",\"TCR\",0,1]\n",
    "label_column = \"ROAS_rate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform em array as variáveis alvo e preditora\n",
    "X = df[training_columns].values\n",
    "y = df[label_column].values\n",
    "\n",
    "# Divida os dados em cojunto de treino teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 28)\n",
    "clf = LassoCV(cv = 10,normalize = True ,random_state = 28)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Realize as predicões\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "# Imprima os coeficientes e as métricas de avaliação\n",
    "print(clf.coef_)\n",
    "print('RMSE: ' + str(mean_squared_error(pred ,y_test)**0.5))\n",
    "print(pearsonr(pred,y_test))\n",
    "print(clf.get_params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma dicionários com os hyperparametros a serem testados do Random Forest\n",
    "param_grid = { \n",
    "    'n_estimators': [100,200],\n",
    "    'max_depth' : [4,5,6,7],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Instancie o modelo em uma váriavel\n",
    "rfc = RandomForestClassifier(random_state = 28)\n",
    "clf = GridSearchCV(estimator = rfc, param_grid = param_grid, cv = 10,scoring  = 'r2')\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "# Imprima os resultados\n",
    "print('RMSE: ' + str(mean_squared_error(pred ,y_test)**0.5))\n",
    "print(pearsonr(pred,y_test))\n",
    "print(clf.get_params)\n",
    "print(clf.best_params_)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
